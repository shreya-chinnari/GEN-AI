{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa186d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jupyterlab notebook\n",
    "! pip install langchain\n",
    "! pip install pinecone-client\n",
    "! pip install pypdf\n",
    "! pip install openai\n",
    "! pip install tiktoken\n",
    "! pip install pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5b58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "import openai\n",
    "import pinecone\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store PDF files\n",
    "!mkdir pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01a39a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all PDF files from the 'pdfs' directory\n",
    "# \"loader\" is an object you created to read your PDF file.\n",
    "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "data = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33fe918e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-07-28T00:37:55+00:00', 'author': 'Engin Iyidogan; Ali I. Ozkes', 'doi': 'https://doi.org/10.48550/arXiv.2507.19183', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-07-28T00:37:55+00:00', 'title': 'Agentic AI and Hallucinations', 'arxivid': 'https://arxiv.org/abs/2507.19183v1', 'source': 'pdfs\\\\Agentic_AI_and_Hallucinations.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='Agentic AI and Hallucinations\\nEngin Iyidogan ∗ ,1 and Ali I. Ozkes 1,2\\n1SKEMA Business School – Universit´ e Cˆ ote d’Azur, Paris, France\\n2WU Vienna University of Economics and Business, Vienna, Austria\\nJuly 28, 2025\\nAbstract\\nWe model a competitive market where AI agents buy answers from u pstream generative\\nmodels and resell them to users who diﬀer in how much they value accu racy and in how much\\nthey fear hallucinations. Agents can privately exert eﬀort for cos tly veriﬁcation to lower hal-\\nlucination risks. Since interactions halt in the event of a hallucination, the threat of losing\\nfuture rents disciplines eﬀort. A unique reputational equilibrium exis ts under nontrivial dis-\\ncounting. The equilibrium eﬀort, and thus the price, increases with t he share of users who have\\nhigh accuracy concerns, implying that hallucination-sensitive secto rs, such as law and medicine,\\nendogenously lead to more serious veriﬁcation eﬀorts in agentic AI m arkets.\\nJEL classiﬁcation: D82, L14, L15, C73\\nKeywords: Agentic AI, Artiﬁcial Intelligence, Hallucination Risk, L arge Language Models\\n∗ Corresponding author.\\nEmail addresses: engin.iyidogan@skema.edu (E. Iyidogan), ali.ozkes@skema.edu (Ali I. Ozkes)\\narXiv:2507.19183v1  [econ.TH]  25 Jul 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-07-28T00:37:55+00:00', 'author': 'Engin Iyidogan; Ali I. Ozkes', 'doi': 'https://doi.org/10.48550/arXiv.2507.19183', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-07-28T00:37:55+00:00', 'title': 'Agentic AI and Hallucinations', 'arxivid': 'https://arxiv.org/abs/2507.19183v1', 'source': 'pdfs\\\\Agentic_AI_and_Hallucinations.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nMany industries increasingly rely on AI agents (\\nBrynjolfsson et al. , 2025; Colback, 2025). These\\nsoftware entities intermediate between upstream general- purpose model providers ( e.g., large lan-\\nguage models, LLMs) and end users by converting queries into structured prompts, routing them\\nto one or several models, and post-processing the returned c ompletions ( Rothschild et al. , 2025).\\nTheir economic role resembles that of traditional data vend ors and consultancy ﬁrms, yet two tech-\\nnological facets distinguish them. First, the upstream sup ply side oﬀers a discrete menu of models\\nthat diﬀer markedly in both their per-query price and their ou tput quality, measured for instance\\nby the baseline hallucination rate, the rate at which the mod el falsely claims to have performed\\nan action or generated a correct response ( Canayaz, 2025; Dahl et al. , 2024). Second, downstream\\nagents can apply various techniques to reduce hallucinatio ns, such as retrieval-augmentation, en-\\nsembling, or human-in-the-loop veriﬁcation ( Shavit et al. , 2023). In other words, AI agents choose\\ntwo quality levers: which model to call and how much eﬀort to ex pend in veriﬁcation.\\nAgents’ quality choices determine the equilibrium mapping from retail price to delivered relia-\\nbility. When hallucination risk is not perfectly internali zed, the standard welfare theorems break\\ndown. In high-stakes domains such as law or medicine, this be comes vitally important ( Asgari\\net al. , 2025). For such users, the disutility from an incorrect answer ex ceeds by orders of magnitude\\nthe service’s sticker price. Users with lower sensitivity t o such failures, such as those who use the\\nservice for entertainment, may rather treat hallucination s as a nuisance. This raises the question:\\nCan competition and relational contracting alone induce AI agents to verify answers to the stan-\\ndard demanded by high-criticality users, and how does the eq uilibrium adjust to the mix of high-\\nversus low-criticality demand?\\nWe develop a tractable discrete-time, inﬁnitely-repeated model in which competitive AI agents\\npick the wholesale model, veriﬁcation eﬀort, and retail pric e. Users diﬀer both in the value they\\nplace on a correct answer and in their aversion to hallucinat ions. Relational contracts make costly\\nveriﬁcation incentive-compatible: the threat of losing fu ture rents disciplines the agent and resolves\\nthe moral-hazard problem. Our model yields testable predic tions on how market composition\\nshapes agentic AI service quality and pricing.\\nThe veriﬁcation mechanism we study is akin to relational-in centive contracts ( Levin, 2003)\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-07-28T00:37:55+00:00', 'author': 'Engin Iyidogan; Ali I. Ozkes', 'doi': 'https://doi.org/10.48550/arXiv.2507.19183', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-07-28T00:37:55+00:00', 'title': 'Agentic AI and Hallucinations', 'arxivid': 'https://arxiv.org/abs/2507.19183v1', 'source': 'pdfs\\\\Agentic_AI_and_Hallucinations.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='in that costly, non-contractible eﬀort is enforced by the thr eat of future lost surplus. In our\\nstudy, that bilateral logic is embedded in a competitive mar ket whose users diﬀer in their tolerance\\nfor hallucinations, so the agent’s informational rent is pa ssed through to prices. Our paper also\\ncomplements the allocation-of-authority framework by Athey et al. (2020). While they study\\nwhether a principal should delegate decision rights to an AI or retain them in-house, we take AI\\nauthority as given and analyze the veriﬁcation eﬀort exerted by a proﬁt-maximizing operator.\\n2 The Model\\nWe develop a dynamic economy that features (i) a unit mass of users with heterogeneous valuation\\nand hallucination aversion, (ii) a perfectly competitive continuum of (AI) agents who assemb le\\ninformation services, and (iii) a ﬁnite set of upstream providers of models ( e.g., LLMs). We\\nmodel the interaction as an inﬁnitely repeated game where un certainty arises from the stochastic\\noccurrence of hallucinations.\\nMore precisely, users and agents interact in a competitive m arket, choosing from a menu of\\nservices oﬀered by model providers. Time is discrete, and all parties share a common discount\\nfactor δ ∈ [0, 1). Each user is endowed with a privately observed type θ ∈ {H(igh), L (ow)}, which\\ngoverns the utility v(θ) > 0 from a correct answer and the disutility α (θ) > 0 from a hallucinated\\nanswer. We impose v(H) > v(L) and, crucially, α (H) > α (L) to capture the heightened stakes of\\nthe segment with high-type users. The population share of H is µ ∈ (0, 1).\\nEach agent is a long-lived ﬁrm that chooses a model m ∈ M, where M is publicly known to\\nbe ﬁnite, to query and a retail price p to post. Although m and p stay the same throughout, the\\nagent chooses its veriﬁcation eﬀort level in each period, e ∈ R≥0. Model m carries a wholesale fee\\nkm > 0 per call and a baseline hallucination probability h0(m) ∈ (0, 1). We assume m ≠m′implies\\nkm ≠km′and h0(m) ≠h0(m′).\\nVeriﬁcation eﬀort, chosen by the agent when interacting with a user, lowers the hallucination\\nprobability exponentially:\\nh(m, e ) = h0(m) exp(−βe), (2.1)\\nwhere β > 0 denotes veriﬁcation eﬃcacy. Eﬀort induces a convex cost c(e) satisfying c(0) = 0,\\nc′(e) >0, and c′′(e) >0 for all e >0.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-07-28T00:37:55+00:00', 'author': 'Engin Iyidogan; Ali I. Ozkes', 'doi': 'https://doi.org/10.48550/arXiv.2507.19183', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-07-28T00:37:55+00:00', 'title': 'Agentic AI and Hallucinations', 'arxivid': 'https://arxiv.org/abs/2507.19183v1', 'source': 'pdfs\\\\Agentic_AI_and_Hallucinations.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content='In any interaction, agents ﬁrst post long-term relational c ontracts, speciﬁed by a model m\\nand a price per-period p. Users observe the available contracts and choose one to ent er a long-\\nterm relationship. Then, in each subsequent period, the use r pays p and the agent chooses an\\nunobservable eﬀort e. After the service is delivered, a hallucination may occur w ith probability\\nh(m, e ). If a hallucination occurs, the user observes this poor outc ome, the relationship terminates,\\nand the user exits the market. If no hallucination occurs, th e relationship continues to the next\\nperiod. In each period, a mass of new, uninformed users enter s the market, equal to the mass of\\nusers who exited in the previous period, keeping the total po pulation constant.\\nA user of type θ receives expected per-period utility\\nUθ(p, m, e ) =(1 −h(m, e ))v(θ)−h(m, e )α (θ)−p.\\nThe total lifetime value to the user from entering the relati onship is the expected discounted sum\\nof these per-period utilities:\\nVθ(p, m, e ) = Uθ(p, m, e )\\n1 −δ(1 −h(m, e )). (2.2)\\nThe user commits to the relationship if and only if their expe cted lifetime value is non-negative,\\nVθ(p, m, e ) ≥0.\\nWe analyze equilibria where agents oﬀer a single, uniﬁed cont ract. In equilibrium, this contract\\nmust be self-sustaining for the agent and acceptable to all p articipating user types. An agent’s\\ncommitment to eﬀort e is sustained by the value of the ongoing relationship. The in centive com-\\npatibility constraint is then\\nc(e) ≤δ[h(m, 0)−h(m, e )]VC , (2.3)\\nwhere VC is the agent’s present value of relational rents, such that\\nVC = p −km −c(e)\\n1 −δ(1 −h(m, e )).\\nCompetition leads agents to lower the price p until this constraint binds, which yields the minimum\\nprice required to sustain eﬀort e:\\np(m, e )=km +c(e)[1 +R(m, e )], (2.4)\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-07-28T00:37:55+00:00', 'author': 'Engin Iyidogan; Ali I. Ozkes', 'doi': 'https://doi.org/10.48550/arXiv.2507.19183', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-07-28T00:37:55+00:00', 'title': 'Agentic AI and Hallucinations', 'arxivid': 'https://arxiv.org/abs/2507.19183v1', 'source': 'pdfs\\\\Agentic_AI_and_Hallucinations.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='where\\nR(m, e ) = 1 −δ(1 −h(m, e ))\\nδ(h(m, 0)−h(m, e )),\\nsuch that c(e)R(m, e ) denotes the agent’s per-period markup (or rent). An equilib rium contract\\nis one that maximizes average user lifetime value, subject t o the enforcement price ( 2.4) and the\\nparticipation constraint, Vθ ≥0, for θ ∈{H, L }.\\n3 Results\\nWe analyze the market outcomes under relational contractin g, focusing on how the value of repu-\\ntation enables veriﬁcation eﬀort in a market with heterogene ous users.\\n3.1 Benchmark: The Spot Market Outcome\\nWe ﬁrst consider the benchmark where future interactions ha ve no value, i.e., δ = 0. For brevity,\\nwe let UH (m, p, e ) ≡Uθ= H (p, m, e ) (and UL(m, p, e ) analogously).\\nProposition 1. If δ = 0, the only sustainable eﬀort level is e∗ = 0. Competitive agents oﬀer\\nthe contract (m∗, p ∗) where p∗= km∗ and the model m∗is the one that maximizes the µ-weighted\\naverage of user utilities:\\nm∗=arg max\\nm∈M\\n{µUH (m, k m, 0)+(1 −µ)UL(m, k m, 0)}\\nThe market is active only if this contract is acceptable to bot h user types.\\nProof. When δ = 0, the IC constraint (\\n2.3) implies e∗= 0. An agent’s cost thus equals km, and\\ncompetition forces the price p∗=km. To attract users, agents must oﬀer a contract that provides t he\\nhighest possible average utility to the market, thus solvin g for m∗as deﬁned in the proposition.\\nThis benchmark highlights that without the shadow of the fut ure, agents have no incentive to\\nexert costly veriﬁcation eﬀort, and the quality of service pr ovision regresses to its minimum possible\\nlevel, e = 0. Competition can only occur over the observable dimension s of baseline model quality\\nh0(m) and its associated cost km, demonstrating that a mechanism like reputation is essenti al to\\nsolving the underlying moral hazard problem.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-07-28T00:37:55+00:00', 'author': 'Engin Iyidogan; Ali I. Ozkes', 'doi': 'https://doi.org/10.48550/arXiv.2507.19183', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-07-28T00:37:55+00:00', 'title': 'Agentic AI and Hallucinations', 'arxivid': 'https://arxiv.org/abs/2507.19183v1', 'source': 'pdfs\\\\Agentic_AI_and_Hallucinations.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='3.2 The Reputational Equilibrium\\nWhen δ > 0, agents can exert positive eﬀort. An equilibrium contract m ust maximize the average\\nuser’s lifetime value, subject to the enforcement price (\\n2.4). This is equivalent to solving a planner’s\\nproblem that maximizes the total surplus allocated to users , net of the incentive rents required by\\nthe agent ( c(e)R(m, e ) discounted). Let vavg (µ)=µv (H)+(1−µ)v(L) and α avg (µ)=µα (H)+(1−\\nµ)α (L). The participation constraint becomes Vθ(m, e ) ≥0, where\\nVθ(m, e ) = (1 −h(m, e ))v(θ)−h(m, e )α (θ)−p(m, e )\\n1 −δ(1 −h(m, e )) ,\\nfor θ ∈{H, L }. The objective is:\\nmax\\nm,e ≥0\\nW (m, e ) = (1 −h(m, e ))vavg (µ)−h(m, e )α avg (µ)−km −c(e)\\n1 −δ(1 −h(m, e )) − c(e)R(m, e )\\n1 −δ(1 −h(m, e )), (3.1)\\nsubject to participation constraints. To solve this, we ﬁrs t identify which constraint is binding in\\nrelation to the equilibrium hallucination probability h∗(m, e ).\\nLemma 1. Let κ = α ( H)−α ( L)\\nv( H)−v( L) be the sensitivity ratio.\\n(i) h∗(m, e ) >1/slash.left( 1 +κ) /Leftrightline⇒ VH (p, m, e ) <VL(p, m, e ), thus, VH ≥0 is binding.\\n(ii) h∗(m, e ) <1/slash.left( 1 +κ) /Leftrightline⇒ VH (p, m, e ) >VL(p, m, e ), thus, VL ≥0 is binding.\\nProof. The ordering of VH and VL is determined by the ordering of the per-period utilities UH and\\nUL. We have UL ≥UH if and only if\\nh(m, e )[α (H)−α (L)] ≥(1 −h(m, e ))[v(H)−v(L)],\\nwhich yields the threshold 1 /slash.left( 1 +κ). The two constraints coincide when h∗(m, e ) =1/slash.left( 1 +κ).\\nThis leads to our main result stated in Theorem 1, which characterizes the equilibrium for\\nnontrivial discount factors. We ﬁrst note that an active equ ilibrium requires that the participation\\nconstraint for the binding users, i.e., the L types, is met. This requires that the per-period utility\\nis suﬃcient to cover the agent’s rent, i.e., (1 −h)vL −hα L −km −c(e) ≥ c(e)R(m, e ). Substituting\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-07-28T00:37:55+00:00', 'author': 'Engin Iyidogan; Ali I. Ozkes', 'doi': 'https://doi.org/10.48550/arXiv.2507.19183', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-07-28T00:37:55+00:00', 'title': 'Agentic AI and Hallucinations', 'arxivid': 'https://arxiv.org/abs/2507.19183v1', 'source': 'pdfs\\\\Agentic_AI_and_Hallucinations.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content='the expression for the rent term R(m, e ) and solving for δ yields the condition δ ≥δ(m, e ), where\\nδ(m, e ) ∶=/parenleft.alt4( 1 −h)+(h0 −h)(1 −h)vL −hα L −km −c(e)\\nc(e) /parenright.alt4\\n−1\\n.\\nThus, an active market is only sustainable if players are suﬃ ciently patient.\\nTheorem 1. An active relational equilibrium contract exists if and onl y if δ ≥δ(m∗, e ∗), where the\\nequilibrium is characterized by the unique pair (m∗, e ∗). The equilibrium veriﬁcation eﬀort e∗is\\nstrictly increasing in the share of high-type users, µ.\\nProof. The equilibrium (m∗, e ∗) solves 3.1 whenever δ ≥ δ(m∗, e ∗). For given m, the ﬁrst-order\\ncondition deﬁnes an interior solution e∗(m)and precisely formulates the trade-oﬀ between generated\\nsurplus and agent’s markup such that\\nd\\nde /parenleft.alt4 (1 −h)vavg (µ)−hα avg (µ)−km −c(e)\\n1 −δ(1 −h) /parenright.alt4 = d\\nde /parenleft.alt4 c(e)\\nδ(h0 −h)/parenright.alt4 , (3.2)\\nwhere h = h(m, e ) and h0 = h0(m). Uniqueness is guaranteed if ∂2W /slash.left ∂e2 < 0, which holds if c′′(e)\\nis suﬃciently large. Diﬀerentiating the ﬁrst-order conditi on with respect to µ, we have\\nde∗\\ndµ = −∂2W /slash.left ∂e∂µ\\n∂2W /slash.left ∂e2 .\\nThe denominator is negative by the second-order condition. The cross-partial derivative becomes:\\n∂2W\\n∂e∂µ = βh ((vH −vL)+(1 −δ)(α H −α L))\\n/parenleft.alt1 1 −δ(1 −h)/parenright.alt1\\n2 ,\\nwhich is positive as vH >vL, α H >α L, β, h >0, and δ ∈(0, 1). It follows that de∗/slash.left dµ >0.\\nOur main contributions are twofold. First, the model highli ghts the trade-oﬀ between a model’s\\nbaseline hallucination risk, h0(m), and an agent’s veriﬁcation eﬀort, e, made visible in both the\\npricing and equilibrium conditions. Second, the comparati ve static ﬁnding de∗/slash.left dµ > 0 delivers a\\nsharp exposition: market composition itself acts as a disci plinary device, in that sectors with higher\\ncomposition of high-criticality users, such as law and medi cine, endogenously generate more heavily\\nveriﬁed and consequently more expensive AI services.\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-07-28T00:37:55+00:00', 'author': 'Engin Iyidogan; Ali I. Ozkes', 'doi': 'https://doi.org/10.48550/arXiv.2507.19183', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-07-28T00:37:55+00:00', 'title': 'Agentic AI and Hallucinations', 'arxivid': 'https://arxiv.org/abs/2507.19183v1', 'source': 'pdfs\\\\Agentic_AI_and_Hallucinations.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='3.3 Numerical Example\\nWe conclude our analysis with a numerical example. Low-type users gain v(L) = 1 from a correct\\nanswer and incur α (L) = 1. 5 if the answer is hallucinated; high-type users gain v(H) = 3 and lose\\nα (H) =10. Veriﬁcation lowers hallucination risk according to (\\n2.1) with eﬃcacy β =0. 70 and incurs\\nquadratic cost c(e) = e2\\n8 . The discount factor is set to δ = 0. 95. The baseline upstream model (A)\\nfeatures a hallucination probability hA\\n0 =0. 20 and wholesale fee kA =0. 05. A second model, Model\\nB, is such that hB\\n0 =0. 13 and kB =0. 30.\\nFigure\\n1a shows that greater patience relaxes the incentive-compati bility constraint, shifting the\\nwhole e∗(µ) schedule upward, while eﬀort is strictly increasing in µ as established in Theorem 1.\\nVeriﬁcation plateaus once e∗≈1. 97, where the hallucination rate falls below 0.02.\\n0.0 0.2 0.4 0.6 0.8 1.0\\n/uni03BC\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\ne\\nμ\\nδ = 0. 75\\nδ = 0.85\\nδ = 0.95\\n(a) Patience and veriﬁcation eﬀort.\\nEquilibrium eﬀort e∗as a function of the share of\\nhigh-type users µ for three discount factors\\n(δ =0. 75, 0. 85, 0. 95).\\n0.0 0.2 0.4 0.6 0.8 1.0\\n/uni03BC\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\ne\\nμ\\nβ = 0.5\\nβ = 0.6\\nβ = 0. 7\\n(b) Veriﬁcation-technology comparative static.\\nEquilibrium eﬀort e∗as a function of µ for three\\nveriﬁcation eﬃcacies ( β =0. 50, 0. 60, 0. 70).\\nFig. 1. Equilibrium veriﬁcation eﬀort under varying discount fact ors and veriﬁcation eﬃcacies.\\nFigure 1b shows that more eﬀective screening technology lifts the e∗(µ) schedule, allowing\\nagents to reach the quality ceiling at lower µ. The result follows from the eﬀort eﬃcacy as in ( 2.1).\\nFigure 2 illustrates the upstream model choice. For µ < 0. 26 agents rely on the cheaper but\\nriskier model and compensate with higher veriﬁcation eﬀort. When the high-type composition\\nexceeds roughly one-fourth of the market, paying the higher wholesale fee for the cleaner model\\nmaximizes surplus, and quality provision shifts from the re tail to the upstream layer.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-07-28T00:37:55+00:00', 'author': 'Engin Iyidogan; Ali I. Ozkes', 'doi': 'https://doi.org/10.48550/arXiv.2507.19183', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-07-28T00:37:55+00:00', 'title': 'Agentic AI and Hallucinations', 'arxivid': 'https://arxiv.org/abs/2507.19183v1', 'source': 'pdfs\\\\Agentic_AI_and_Hallucinations.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9'}, page_content='0.0 0.2 0.4 0.6 0.8 1.0\\n/uni03BC\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nW\\nModelμAμ( h\\n0\\n= 0. 20,μ k = 0.05)\\nModelμBμ( h\\n0\\n= 0.13,μ k = 0.30)\\nFig. 2. Upstream model choice.Notes: Total welfare W under Model A ( hA\\n0 =0.20, kA =0.05, solid) and Model B\\n(hB\\n0 =0.13, kB =0.30, dashed).\\nReferences\\nAsgari, E., N. Monta˜ na-Brown, M. Dubois, S. Khalil, J. Ball och, J. A. Yeung, and D. Pimenta\\n(2025). A Framework to Assess Clinical Safety and Hallucina tion Rates of LLMs for Medical\\nText Summarisation.\\nnpj Digital Medicine 8(1), 274.\\nAthey, S. C., K. A. Bryan, and J. S. Gans (2020). The allocatio n of decision authority to human\\nand artiﬁcial intelligence. AEA Papers and Proceedings 110, 80–84.\\nBrynjolfsson, E., D. Li, and L. Raymond (2025). Generative A I at Work. The Quarterly Journal\\nof Economics 140(2), 889–942.\\nCanayaz, M. (2025). AI Agency. Available at SSRN 5109326.\\nColback, L. (2025). AI Agents: From Co-pilot to Autopilot.\\nhttps://www.ft.com/content/3e862e23-6e2c-4670-a68c- e204379fe01f. [Accessed 22-\\n07-2025].\\nDahl, M., V. Magesh, M. Suzgun, and D. E. Ho (2024). Large Lega l Fictions: Proﬁling Legal\\nHallucinations in Large Language Models. Journal of Legal Analysis 16(1), 64–93.\\nLevin, J. (2003). Relational incentive contracts. American Economic Review 93(3), 835–857.\\nRothschild, D. M., M. Mobius, J. M. Hofman, E. W. Dillon, D. G. Goldstein, N. Immorlica,\\nS. Jaﬀe, B. Lucier, A. Slivkins, and M. Vogel (2025). The Agent ic Economy. arXiv preprint\\narXiv:2505.15799.\\nShavit, Y., S. Agarwal, M. Brundage, S. Adler, C. O’Keefe, R. Campbell, T. Lee, P. Mishkin,\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-07-28T00:37:55+00:00', 'author': 'Engin Iyidogan; Ali I. Ozkes', 'doi': 'https://doi.org/10.48550/arXiv.2507.19183', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-07-28T00:37:55+00:00', 'title': 'Agentic AI and Hallucinations', 'arxivid': 'https://arxiv.org/abs/2507.19183v1', 'source': 'pdfs\\\\Agentic_AI_and_Hallucinations.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10'}, page_content='T. Eloundou, A. Hickey, et al. (2023). Practices for Governi ng Agentic AI Systems. Research\\nPaper, OpenAI.\\n10')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1291d",
   "metadata": {},
   "source": [
    "# Purpose: Load a PDF and turn it into a list of “documents”. \n",
    "In LangChain, a Document is a Python object that contains:\n",
    "\n",
    "> page_content: the raw text content of a page.   \n",
    "> metadata: optional information like page number, source filename, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1a89537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI and Hallucinations\n",
      "Engin Iyidogan ∗ ,1 and Ali I. Ozkes 1,2\n",
      "1SKEMA Business School – Universit´ e Cˆ ote d’Azur, Paris, France\n",
      "2WU Vienna University of Economics and Business, Vienna, Austria\n",
      "July 28, 2025\n",
      "Abstract\n",
      "We model a competitive market where AI agents buy answers from u pstream generative\n",
      "models and resell them to users who diﬀer in how much they value accu racy and in how much\n",
      "they fear hallucinations. Agents can privately exert eﬀort for cos tly veriﬁcation to lower hal-\n",
      "lucination risks. Since interactions halt in the event of a hallucination, the threat of losing\n",
      "future rents disciplines eﬀort. A unique reputational equilibrium exis ts under nontrivial dis-\n",
      "counting. The equilibrium eﬀort, and thus the price, increases with t he share of users who have\n",
      "high accuracy concerns, implying that hallucination-sensitive secto rs, such as law and medicine,\n",
      "endogenously lead to more serious veriﬁcation eﬀorts in agentic AI m arkets.\n",
      "JEL classiﬁcation: D82, L14, L15, C73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load all pages as documents\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Inspect first page content\n",
    "print(pages[0].page_content[:1000])  # first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36ef4567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 70\n",
      "Agentic AI and Hallucinations\n",
      "Engin Iyidogan ∗ ,1 and Ali I. Ozkes 1,2\n",
      "1SKEMA Business School – Universit´ e Cˆ ote d’Azur, Paris, France\n",
      "2WU Vienna University of Economics and Business, Vienna, Austria\n",
      "July 28, 2025\n",
      "Abstract\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # A class from LangChain that splits large text into smaller chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,      # 300 characters per chunk\n",
    "    chunk_overlap=20     # 20 characters overlap between chunks\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "# Check number of chunks\n",
    "print(f\"Total chunks: {len(text_chunks)}\")\n",
    "\n",
    "# Inspect first chunk\n",
    "print(text_chunks[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9445dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model the interaction as an inﬁnitely repeated game where un certainty arises from the stochastic\n",
      "occurrence of hallucinations.\n",
      "More precisely, users and agents interact in a competitive m arket, choosing from a menu of\n"
     ]
    }
   ],
   "source": [
    "# Inspect 20th chunk\n",
    "print(text_chunks[19].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819c0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T. Eloundou, A. Hickey, et al. (2023). Practices for Governi ng Agentic AI Systems. Research\n",
      "Paper, OpenAI.\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Inspect last chunk\n",
    "print(text_chunks[-1].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai2)",
   "language": "python",
   "name": "genai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
